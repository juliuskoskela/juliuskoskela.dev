<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Intel SIMD Study | Julius Koskela</title>
<meta name="keywords" content="">
<meta name="description" content="Optimizing Simple Matrix Multiplication With Intel SIMD Abstract This is a test report about possible performance gains in a simple 4x4 or 8x8 matrix multiplication when using Intel&rsquo;s intrinsic instruction wrappers and SIMD (Single Instruction Mutiple Data) types. SIMD intrinsic functions are basically wrappers for assembly instructions, which can be conveniently used inside a C-program. Thus we can explore the full power of vectorization and other techniques taking advantage of the wider datatypes and corresponding methods.">
<meta name="author" content="">
<link rel="canonical" href="https://juliuskoskela.dev/posts/intel-simd-study/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.ed52a04cba0843fef8297e018b15e8a32a989ea4415133cb8bf77414d3815f7b.css" integrity="sha256-7VKgTLoIQ/74KX4BixXooyqYnqRBUTPLi/d0FNOBX3s=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js" integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://juliuskoskela.dev/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://juliuskoskela.dev/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://juliuskoskela.dev/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://juliuskoskela.dev/apple-touch-icon.png">
<link rel="mask-icon" href="https://juliuskoskela.dev/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Intel SIMD Study" />
<meta property="og:description" content="Optimizing Simple Matrix Multiplication With Intel SIMD Abstract This is a test report about possible performance gains in a simple 4x4 or 8x8 matrix multiplication when using Intel&rsquo;s intrinsic instruction wrappers and SIMD (Single Instruction Mutiple Data) types. SIMD intrinsic functions are basically wrappers for assembly instructions, which can be conveniently used inside a C-program. Thus we can explore the full power of vectorization and other techniques taking advantage of the wider datatypes and corresponding methods." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://juliuskoskela.dev/posts/intel-simd-study/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-04-22T00:39:23&#43;03:00" />
<meta property="article:modified_time" content="2022-04-22T00:39:23&#43;03:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Intel SIMD Study"/>
<meta name="twitter:description" content="Optimizing Simple Matrix Multiplication With Intel SIMD Abstract This is a test report about possible performance gains in a simple 4x4 or 8x8 matrix multiplication when using Intel&rsquo;s intrinsic instruction wrappers and SIMD (Single Instruction Mutiple Data) types. SIMD intrinsic functions are basically wrappers for assembly instructions, which can be conveniently used inside a C-program. Thus we can explore the full power of vectorization and other techniques taking advantage of the wider datatypes and corresponding methods."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://juliuskoskela.dev/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Intel SIMD Study",
      "item": "https://juliuskoskela.dev/posts/intel-simd-study/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Intel SIMD Study",
  "name": "Intel SIMD Study",
  "description": "Optimizing Simple Matrix Multiplication With Intel SIMD Abstract This is a test report about possible performance gains in a simple 4x4 or 8x8 matrix multiplication when using Intel\u0026rsquo;s intrinsic instruction wrappers and SIMD (Single Instruction Mutiple Data) types. SIMD intrinsic functions are basically wrappers for assembly instructions, which can be conveniently used inside a C-program. Thus we can explore the full power of vectorization and other techniques taking advantage of the wider datatypes and corresponding methods.",
  "keywords": [
    
  ],
  "articleBody": "Optimizing Simple Matrix Multiplication With Intel SIMD Abstract This is a test report about possible performance gains in a simple 4x4 or 8x8 matrix multiplication when using Intel’s intrinsic instruction wrappers and SIMD (Single Instruction Mutiple Data) types. SIMD intrinsic functions are basically wrappers for assembly instructions, which can be conveniently used inside a C-program. Thus we can explore the full power of vectorization and other techniques taking advantage of the wider datatypes and corresponding methods.\nSIMD: A Brief Introduction SIMD stands for single instruction multiple data and means exactly that. We operate on data registers that are wider than those commonly used in programs and which can hold up to 512-bits of data instead of a maximum of 64-bits. We get instructions which operate on such registers simultaneously.\nEach CPU instruction set has their own corresponding SIMD-instructions. Different architectures may or may not prowide datatypes with the same sizes. In this article we concentrate on Intel x86 instruction set and it’s corresponding instrinsics. When we implement something with these teachniques, we are implementing hardware dependant code, which will not compile for platforms using different instruction sets such as ARM or RISC-V. However we can implement SIMD optimizations on top of naive implementations in such a way that the optimizations are used only if the target platform supports them and otheriwse we use the naive implementations as fallback and let the compiler optimize them as best it can.\nSIMD - Vectors The first thing we need to understand is that the SIMD registers work as fixed size vectors. So a __m128 type is the same as float[4] and typecasting between the two or putting them in a union works.\n __m128\txmm;\t// A 128-bit register of 4 x float __m128d\txmm;\t// A 128-bit register of 2 x double __m128i\txmm;\t// A 128-bit register of 4 x int32?? __m256\txmm;\t// A 256-bit register of 8 x float __m256d\txmm;\t// A 256-bit register of 4 x double __m256i\txmm;\t// A 256-bit register of 4 x int64 Convenient Unions In my tests I found a convenient use for unions in the context of SIMD vectors.\n typedef union u_m4x4f { \tfloat\tn[4][4]; \t__m128\txmm[4]; }\tt_m4x4f; The __m128 type is a 128-bit wide register interpreted as a float. In order to produce a 4x4 matrix, we need 4 such registers. When we declare those registers inside a union together with a 4x4 array of floats, we can easily access the values in the matrix without having to do ugly casts.\nBasic Operations Let’s consider simple addition of elements in two vectors. We could use the function _mm_add_ps to do this.\n 1 2 3 4 + + + + 2 3 4 5 = = = = 2 6 12 20 Vectorization Vectorization is a method of parallel computation. When working with SIMD- types, we leverage the fact that we have up to 512-bit (in AVX512 enabled machines) wide registers which we can operate on. However there are also clear limitations to this method as will be explored later.\nThe first bit of important insight we need to establish is where in the process we actually find the performance gains. Let’s consider multiplying two vectors of floats together.\n void mul(float dst[4], float a[4], float b[4]) { \tfor (i = 0; i  4; i++) \t{ \tdst[i] = a[i] * b[i]; \t} } Above we iterate over the values and multiply them together. Now let’s do the same with SIMD registers.\n __m128 mul(__m128 dst, __m128 a, __m128 b) { \treturn (__mm_mul_ps(a, b); } Actually the whole function is redundant because the __mm_mul_ps itself is the multiplication function for the whole vector of 4 floats.\nSo how will this lead to a performance increase? Well multiplying 2 floats together with the * operator will take about the same amount of clock-cycles as multiplying those 8 floats in the two SIMD registers together. Furthermore we have even bigger SIMD-registers representing 8 or even 16 floats. If we can structure our logic such that we do as much parallel computation as possible, we can get performance gains against a linear algorithm.\nIt is important to mention that this is exactly what the compiler does as a part of the optimization steps when you optimize your program with the -O3 flag.\nFuthermore it’s worth mentioning that the same logic of vectorization is closely related to the technique of loop unrolling.\n!!!Talk about how parallel vs. linear computing methods partly solve different problems. For example dot product.\nMatrix Multiplication Naive Implementation  t_m4x4f\tm4x4f_mul_naive(t_m4x4f *a, t_m4x4f *b) { \tt_m4x4f\tres;  \tfor(int i = 0; i  4; i++) \t{ \tfor(int j = 0; j  4; j++) \t{ \tres.n[i][j] = 0; \tfor(int k = 0; k  4; k++) \t{ \tres.n[i][j] += a-n[i][k] * b-n[k][j]; \t} \t} \t} \treturn (res); } Simple Implementation Using SIMD  t_m4x4f\tm4x4f_mul_direct_no_unroll(t_m4x4f *a, t_m4x4f *b) { \tt_m4x4f\tres; \t__m128\txmm[4]; \tuint8_t\ti;  \ti = 0; \twhile (i  4) \t{ \txmm[0] = _mm_broadcast_ss(\u0026a-n[i][0]); \txmm[1] = _mm_broadcast_ss(\u0026a-n[i][1]); \txmm[2] = _mm_broadcast_ss(\u0026a-n[i][2]); \txmm[3] = _mm_broadcast_ss(\u0026a-n[i][3]); \tres.xmm[i] = _mm_mul_ps(xmm[0], b-xmm[0]); \tres.xmm[i] = _mm_fmadd_ps(xmm[1], b-xmm[1], res.xmm[i]); \tres.xmm[i] = _mm_fmadd_ps(xmm[2], b-xmm[2], res.xmm[i]); \tres.xmm[i] = _mm_fmadd_ps(xmm[3], b-xmm[3], res.xmm[i]); \ti++; \t} \treturn (res); } Test and Results Test Setup Results Conclusions Resources https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/\n",
  "wordCount" : "873",
  "inLanguage": "en",
  "datePublished": "2022-04-22T00:39:23+03:00",
  "dateModified": "2022-04-22T00:39:23+03:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://juliuskoskela.dev/posts/intel-simd-study/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Julius Koskela",
    "logo": {
      "@type": "ImageObject",
      "url": "https://juliuskoskela.dev/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://juliuskoskela.dev" accesskey="h" title="Julius Koskela (Alt + H)">Julius Koskela</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Intel SIMD Study
    </h1>
    <div class="post-meta"><span title='2022-04-22 00:39:23 +0300 EEST'>April 22, 2022</span>

</div>
  </header> 
  <div class="post-content"><h2 id="optimizing-simple-matrix-multiplication-with-intel-simd">Optimizing Simple Matrix Multiplication With Intel SIMD<a hidden class="anchor" aria-hidden="true" href="#optimizing-simple-matrix-multiplication-with-intel-simd">#</a></h2>
<h3 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h3>
<p>This is a test report about possible performance gains in a simple 4x4 or 8x8
matrix multiplication when using Intel&rsquo;s intrinsic instruction wrappers and SIMD
(Single Instruction Mutiple Data) types.  SIMD intrinsic functions are basically
wrappers for assembly instructions, which can be conveniently used inside a
C-program. Thus we can explore the full power of vectorization and other
techniques taking advantage of the wider datatypes and corresponding methods.</p>
<h3 id="simd-a-brief-introduction">SIMD: A Brief Introduction<a hidden class="anchor" aria-hidden="true" href="#simd-a-brief-introduction">#</a></h3>
<p>SIMD stands for single instruction multiple data and means exactly that. We
operate on data registers that are wider than those commonly used in programs
and which can hold up to 512-bits of data instead of a maximum of 64-bits. We
get instructions which operate on such registers simultaneously.</p>
<p>Each CPU instruction set has their own corresponding SIMD-instructions.
Different architectures may or may not prowide datatypes with the same sizes. In
this article we concentrate on Intel x86 instruction set and it&rsquo;s corresponding
instrinsics. When we implement something with these teachniques, we are
implementing hardware dependant code, which will not compile for platforms using
different instruction sets such as ARM or RISC-V. However we can implement SIMD
optimizations on top of naive implementations in such a way that the
optimizations are used only if the target platform supports them and otheriwse
we use the naive implementations as fallback and let the compiler optimize them
as best it can.</p>
<h4 id="simd---vectors">SIMD - Vectors<a hidden class="anchor" aria-hidden="true" href="#simd---vectors">#</a></h4>
<p>The first thing we need to understand is that the SIMD registers work as fixed
size vectors. So a <code>__m128</code> type is the same as <code>float[4]</code> and typecasting
between the two or putting them in a union works.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">__m128</span>	xmm;	<span style="color:#75715e">// A 128-bit register of 4 x float
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">__m128d</span>	xmm;	<span style="color:#75715e">// A 128-bit register of 2 x double
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">__m128i</span>	xmm;	<span style="color:#75715e">// A 128-bit register of 4 x int32??
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>__m256	xmm;	<span style="color:#75715e">// A 256-bit register of 8 x float
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>__m256d	xmm;	<span style="color:#75715e">// A 256-bit register of 4 x double
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>__m256i	xmm;	<span style="color:#75715e">// A 256-bit register of 4 x int64
</span></span></span></code></pre></div><h4 id="convenient-unions">Convenient Unions<a hidden class="anchor" aria-hidden="true" href="#convenient-unions">#</a></h4>
<p>In my tests I found a convenient use for unions in the context of SIMD
vectors.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">typedef</span> <span style="color:#66d9ef">union</span> u_m4x4f
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">float</span>	n[<span style="color:#ae81ff">4</span>][<span style="color:#ae81ff">4</span>];
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">__m128</span>	xmm[<span style="color:#ae81ff">4</span>];
</span></span><span style="display:flex;"><span>}	t_m4x4f;
</span></span></code></pre></div><p>The <code>__m128</code> type is a 128-bit wide register interpreted as a float. In order to
produce a 4x4 matrix, we need 4 such registers. When we declare those registers
inside a union together with a 4x4 array of floats, we can easily access the
values in the matrix without having to do ugly casts.</p>
<h4 id="basic-operations">Basic Operations<a hidden class="anchor" aria-hidden="true" href="#basic-operations">#</a></h4>
<p>Let&rsquo;s consider simple addition of elements in two vectors. We could use the
function <code>_mm_add_ps</code> to do this.</p>
<pre tabindex="0"><code>
1  2  3  4
+  +  +  +
2  3  4  5
=  =  =  =
2  6  12 20
</code></pre><h3 id="vectorization">Vectorization<a hidden class="anchor" aria-hidden="true" href="#vectorization">#</a></h3>
<p>Vectorization is a method of parallel computation. When working with SIMD-
types, we leverage the fact that we have up to 512-bit (in AVX512 enabled
machines) wide registers which we can operate on. However there are also clear
limitations to this method as will be explored later.</p>
<p>The first bit of important insight we need to establish is where in the process
we actually find the performance gains. Let&rsquo;s consider multiplying two vectors
of floats together.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">mul</span>(<span style="color:#66d9ef">float</span> dst[<span style="color:#ae81ff">4</span>], <span style="color:#66d9ef">float</span> a[<span style="color:#ae81ff">4</span>], <span style="color:#66d9ef">float</span> b[<span style="color:#ae81ff">4</span>])
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">for</span> (i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">4</span>; i<span style="color:#f92672">++</span>)
</span></span><span style="display:flex;"><span>	{
</span></span><span style="display:flex;"><span>		dst[i] <span style="color:#f92672">=</span> a[i] <span style="color:#f92672">*</span> b[i];
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Above we iterate over the values and multiply them together. Now let&rsquo;s do the
same with SIMD registers.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">__m128</span> <span style="color:#a6e22e">mul</span>(<span style="color:#66d9ef">__m128</span> dst, <span style="color:#66d9ef">__m128</span> a, <span style="color:#66d9ef">__m128</span> b)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> (__mm_mul_ps(a, b);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Actually the whole function is redundant because the <code>__mm_mul_ps</code> itself is the
multiplication function for the whole vector of 4 floats.</p>
<p>So how will this lead to a performance increase? Well multiplying 2 floats
together with the <code>*</code> operator will take about the same amount of clock-cycles
as multiplying those 8 floats in the two SIMD registers together. Furthermore we
have even bigger SIMD-registers representing 8 or even 16 floats. If we can
structure our logic such that we do as much parallel computation as possible, we
can get performance gains against a linear algorithm.</p>
<p>It is important to mention that this is exactly what the compiler does as a part
of the optimization steps when you optimize your program with the <code>-O3</code> flag.</p>
<p>Futhermore it&rsquo;s worth mentioning that the same logic of vectorization is closely
related to the technique of loop unrolling.</p>
<p>!!!Talk about how parallel vs. linear computing methods partly solve different
problems. For example dot product.</p>
<h3 id="matrix-multiplication">Matrix Multiplication<a hidden class="anchor" aria-hidden="true" href="#matrix-multiplication">#</a></h3>
<h4 id="naive-implementation">Naive Implementation<a hidden class="anchor" aria-hidden="true" href="#naive-implementation">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>t_m4x4f	<span style="color:#a6e22e">m4x4f_mul_naive</span>(t_m4x4f <span style="color:#f92672">*</span>a, t_m4x4f <span style="color:#f92672">*</span>b)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>	t_m4x4f	res;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">for</span>(<span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">4</span>; i<span style="color:#f92672">++</span>)
</span></span><span style="display:flex;"><span>	{
</span></span><span style="display:flex;"><span>	    <span style="color:#66d9ef">for</span>(<span style="color:#66d9ef">int</span> j <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; j <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">4</span>; j<span style="color:#f92672">++</span>)
</span></span><span style="display:flex;"><span>		{
</span></span><span style="display:flex;"><span>	        res.n[i][j] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>	        <span style="color:#66d9ef">for</span>(<span style="color:#66d9ef">int</span> k <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; k <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">4</span>; k<span style="color:#f92672">++</span>)
</span></span><span style="display:flex;"><span>			{
</span></span><span style="display:flex;"><span>	            res.n[i][j] <span style="color:#f92672">+=</span> a<span style="color:#f92672">-&gt;</span>n[i][k] <span style="color:#f92672">*</span> b<span style="color:#f92672">-&gt;</span>n[k][j];
</span></span><span style="display:flex;"><span>	        }
</span></span><span style="display:flex;"><span>	    }
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> (res);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h4 id="simple-implementation-using-simd">Simple Implementation Using SIMD<a hidden class="anchor" aria-hidden="true" href="#simple-implementation-using-simd">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>t_m4x4f	<span style="color:#a6e22e">m4x4f_mul_direct_no_unroll</span>(t_m4x4f <span style="color:#f92672">*</span>a, t_m4x4f <span style="color:#f92672">*</span>b)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>	t_m4x4f	res;
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">__m128</span>	xmm[<span style="color:#ae81ff">4</span>];
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">uint8_t</span>	i;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">while</span> (i <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>	{
</span></span><span style="display:flex;"><span>		xmm[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> _mm_broadcast_ss(<span style="color:#f92672">&amp;</span>a<span style="color:#f92672">-&gt;</span>n[i][<span style="color:#ae81ff">0</span>]);
</span></span><span style="display:flex;"><span>		xmm[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> _mm_broadcast_ss(<span style="color:#f92672">&amp;</span>a<span style="color:#f92672">-&gt;</span>n[i][<span style="color:#ae81ff">1</span>]);
</span></span><span style="display:flex;"><span>		xmm[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> _mm_broadcast_ss(<span style="color:#f92672">&amp;</span>a<span style="color:#f92672">-&gt;</span>n[i][<span style="color:#ae81ff">2</span>]);
</span></span><span style="display:flex;"><span>		xmm[<span style="color:#ae81ff">3</span>] <span style="color:#f92672">=</span> _mm_broadcast_ss(<span style="color:#f92672">&amp;</span>a<span style="color:#f92672">-&gt;</span>n[i][<span style="color:#ae81ff">3</span>]);
</span></span><span style="display:flex;"><span>		res.xmm[i] <span style="color:#f92672">=</span> _mm_mul_ps(xmm[<span style="color:#ae81ff">0</span>], b<span style="color:#f92672">-&gt;</span>xmm[<span style="color:#ae81ff">0</span>]);
</span></span><span style="display:flex;"><span>		res.xmm[i] <span style="color:#f92672">=</span> _mm_fmadd_ps(xmm[<span style="color:#ae81ff">1</span>], b<span style="color:#f92672">-&gt;</span>xmm[<span style="color:#ae81ff">1</span>], res.xmm[i]);
</span></span><span style="display:flex;"><span>		res.xmm[i] <span style="color:#f92672">=</span> _mm_fmadd_ps(xmm[<span style="color:#ae81ff">2</span>], b<span style="color:#f92672">-&gt;</span>xmm[<span style="color:#ae81ff">2</span>], res.xmm[i]);
</span></span><span style="display:flex;"><span>		res.xmm[i] <span style="color:#f92672">=</span> _mm_fmadd_ps(xmm[<span style="color:#ae81ff">3</span>], b<span style="color:#f92672">-&gt;</span>xmm[<span style="color:#ae81ff">3</span>], res.xmm[i]);
</span></span><span style="display:flex;"><span>		i<span style="color:#f92672">++</span>;
</span></span><span style="display:flex;"><span>	}
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> (res);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h3 id="test-and-results">Test and Results<a hidden class="anchor" aria-hidden="true" href="#test-and-results">#</a></h3>
<h4 id="test-setup">Test Setup<a hidden class="anchor" aria-hidden="true" href="#test-setup">#</a></h4>
<h4 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h4>
<h4 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h4>
<h3 id="resources">Resources<a hidden class="anchor" aria-hidden="true" href="#resources">#</a></h3>
<p><a href="https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/">https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/</a></p>


  </div>

  <footer class="post-footer">
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://juliuskoskela.dev">Julius Koskela</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
